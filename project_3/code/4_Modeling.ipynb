{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Modeling\" data-toc-modified-id=\"Modeling-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Modeling</a></span><ul class=\"toc-item\"><li><span><a href=\"#Using-the-TF-IDF-Vectorizer\" data-toc-modified-id=\"Using-the-TF-IDF-Vectorizer-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Using the TF-IDF Vectorizer</a></span><ul class=\"toc-item\"><li><span><a href=\"#And-classified-with-Logistic-Regression\" data-toc-modified-id=\"And-classified-with-Logistic-Regression-5.1.1\"><span class=\"toc-item-num\">5.1.1&nbsp;&nbsp;</span>And classified with Logistic Regression</a></span></li><li><span><a href=\"#And-classified-with-Multinomial-Naive-Bayes\" data-toc-modified-id=\"And-classified-with-Multinomial-Naive-Bayes-5.1.2\"><span class=\"toc-item-num\">5.1.2&nbsp;&nbsp;</span>And classified with Multinomial Naive Bayes</a></span></li></ul></li><li><span><a href=\"#With-CountVectorizer\" data-toc-modified-id=\"With-CountVectorizer-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>With CountVectorizer</a></span><ul class=\"toc-item\"><li><span><a href=\"#With-Logistic-Regression\" data-toc-modified-id=\"With-Logistic-Regression-5.2.1\"><span class=\"toc-item-num\">5.2.1&nbsp;&nbsp;</span>With Logistic Regression</a></span></li><li><span><a href=\"#With-Multinomial-Naive-Bayes\" data-toc-modified-id=\"With-Multinomial-Naive-Bayes-5.2.2\"><span class=\"toc-item-num\">5.2.2&nbsp;&nbsp;</span>With Multinomial Naive Bayes</a></span></li></ul></li></ul></li><li><span><a href=\"#Evaluating-the-best-model\" data-toc-modified-id=\"Evaluating-the-best-model-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Evaluating the best model</a></span><ul class=\"toc-item\"><li><span><a href=\"#Saving-Features-of-gs3\" data-toc-modified-id=\"Saving-Features-of-gs3-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Saving Features of gs3</a></span></li><li><span><a href=\"#Saving-Features-from-gs2\" data-toc-modified-id=\"Saving-Features-from-gs2-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>Saving Features from gs2</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "# Import CountVectorizer and TFIDFVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Import Classifiers\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../datasets/adhd_ocd_210313_mod.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "subreddit      0\n",
       "content_mod    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for NaNs before beginning\n",
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.51547\n",
       "0    0.48453\n",
       "Name: subreddit, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate baseline to beat\n",
    "data['subreddit'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling\n",
    "\n",
    "For create our best model, we will attempt to use two vectorizers available in sklearn:\n",
    "1. TF-IDF Vectorizer\n",
    "2. Count Vectorizer\n",
    "\n",
    "It is expected that TF-IDF Vectorizer models will perform better than Count Vectorizer ones due to its ability to penalize more frequently occuring words, which from our previous section we already know that r/ADHD and r/OCD share many of.\n",
    "\n",
    "We will also employ two classifiers:\n",
    "1. Naive Bayes\n",
    "2. Logistic Regression\n",
    "\n",
    "For our purposes, a Type I and Type II error are equal in severity (neither are worse than the other). Thus, we will be optimizing our models for the highest possible accuracy while attempting to ensure that it is not overfitted on the training data.\n",
    "\n",
    "We will find the best parameters for each of our models using GridSearchCV and making changes as needed. As with the last section, r/ADHD and r/OCD posts share a high number of words used. We expect that our models will utilise less than 50% of available features for prediction, and also omit many most frequently seen words.\n",
    "\n",
    "For the purposes of our project -- since we are concerned about indivudual words, we will be keeping all vectorizers' ngram_ranges at (1, 1) and ignore composite words.\n",
    "\n",
    "We will be using 25% of our datasets rows as a test set and the remaining as the training set.\n",
    "\n",
    "Our baseline model to beat as indicated above, is .51 or 51% accuracy from predicting that every post belongs to r/OCD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results will be saved in a table and compared at the end\n",
    "results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the TF-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data['content_mod']\n",
    "y = data['subreddit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                   test_size=.25,\n",
    "                                                    random_state = 42,\n",
    "                                                   stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### And classified with Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    ('vector', TfidfVectorizer(stop_words='english')),\n",
    "    ('lgr', LogisticRegression(solver='lbfgs')),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_params = {\n",
    "    'vector__max_features': [2500, 3000, 3500],\n",
    "    'vector__min_df': [2, 3, 4],\n",
    "    'vector__max_df': [.25, .3, .35],\n",
    "    'lgr__max_iter': [10000],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs = GridSearchCV(pipe, \n",
    "                  pipe_params,\n",
    "                  cv=5,\n",
    "                  n_jobs=-1,\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('vector',\n",
       "                                        TfidfVectorizer(stop_words='english')),\n",
       "                                       ('lgr', LogisticRegression())]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'lgr__max_iter': [10000],\n",
       "                         'vector__max_df': [0.25, 0.3, 0.35],\n",
       "                         'vector__max_features': [2500, 3000, 3500],\n",
       "                         'vector__min_df': [2, 3, 4]})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vector',\n",
       "                 TfidfVectorizer(max_df=0.3, max_features=3000, min_df=3,\n",
       "                                 stop_words='english')),\n",
       "                ('lgr', LogisticRegression(max_iter=10000))])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.update({'gs_TF_Lgr':\n",
    "               {\n",
    "                   'train_score': gs.best_score_,\n",
    "                   'test_score': gs.score(X_test, y_test)\n",
    "               }})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### And classified with Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe2 = Pipeline([\n",
    "    ('vector', TfidfVectorizer(stop_words='english')),\n",
    "    ('nb', MultinomialNB()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_params2 = {\n",
    "    'vector__max_features': [3000, 3500, 4000],\n",
    "    'vector__min_df': [1, 2],\n",
    "    'vector__max_df': [.3, .35, .4],\n",
    "    'nb__alpha': [.4, .5, .6]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs2 = GridSearchCV(pipe2, # what object are we optimizing?\n",
    "                  pipe_params2, # what parameters values are we searching?\n",
    "                  cv=5,\n",
    "                n_jobs=-1,\n",
    "                 ) # 5-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('vector',\n",
       "                                        TfidfVectorizer(stop_words='english')),\n",
       "                                       ('nb', MultinomialNB())]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'nb__alpha': [0.4, 0.5, 0.6],\n",
       "                         'vector__max_df': [0.3, 0.35, 0.4],\n",
       "                         'vector__max_features': [3000, 3500, 4000],\n",
       "                         'vector__min_df': [1, 2]})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vector',\n",
       "                 TfidfVectorizer(max_df=0.4, max_features=3000, min_df=2,\n",
       "                                 stop_words='english')),\n",
       "                ('nb', MultinomialNB(alpha=0.4))])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs2.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.update({'gs2_TF_NB':\n",
    "               {\n",
    "                   'train_score': gs2.best_score_,\n",
    "                   'test_score': gs2.score(X_test, y_test)\n",
    "               }})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### With Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe3 = Pipeline([\n",
    "    ('vector', CountVectorizer(stop_words='english')),\n",
    "    ('lgr', LogisticRegression()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_params3 = {\n",
    "    'vector__max_features': [2500, 3000, 3500],\n",
    "    'vector__min_df': [2, 3],\n",
    "    'vector__max_df': [.3, .35, .4],\n",
    "    'lgr__max_iter': [10000],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs3 = GridSearchCV(pipe3, \n",
    "                  pipe_params3,\n",
    "                  cv=5,\n",
    "                   n_jobs=-1,\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('vector',\n",
       "                                        CountVectorizer(stop_words='english')),\n",
       "                                       ('lgr', LogisticRegression())]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'lgr__max_iter': [10000],\n",
       "                         'vector__max_df': [0.3, 0.35, 0.4],\n",
       "                         'vector__max_features': [2500, 3000, 3500],\n",
       "                         'vector__min_df': [2, 3]})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs3.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vector',\n",
       "                 CountVectorizer(max_df=0.35, max_features=3500, min_df=2,\n",
       "                                 stop_words='english')),\n",
       "                ('lgr', LogisticRegression(max_iter=10000))])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs3.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.update({'gs3_Count_Lgr':\n",
    "               {\n",
    "                   'train_score': gs3.best_score_,\n",
    "                   'test_score': gs3.score(X_test, y_test)\n",
    "               }})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### With Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe4 = Pipeline([\n",
    "    ('vector', CountVectorizer(stop_words='english')),\n",
    "    ('nb', MultinomialNB()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_params4 = {\n",
    "    'vector__max_features': [1000, 1500],\n",
    "    'vector__min_df': [1, 2],\n",
    "    'vector__max_df': [.25, .3, .35],\n",
    "    'nb__alpha': [.3, .4, .5],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs4 = GridSearchCV(pipe4, \n",
    "                  pipe_params4,\n",
    "                  cv=5,\n",
    "                   n_jobs=-1,\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('vector',\n",
       "                                        CountVectorizer(stop_words='english')),\n",
       "                                       ('nb', MultinomialNB())]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'nb__alpha': [0.3, 0.4, 0.5],\n",
       "                         'vector__max_df': [0.25, 0.3, 0.35],\n",
       "                         'vector__max_features': [1000, 1500],\n",
       "                         'vector__min_df': [1, 2]})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs4.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vector',\n",
       "                 CountVectorizer(max_df=0.3, max_features=1000,\n",
       "                                 stop_words='english')),\n",
       "                ('nb', MultinomialNB(alpha=0.4))])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs4.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.update({'gs4_Count_NB':\n",
    "               {\n",
    "                   'train_score': gs4.best_score_,\n",
    "                   'test_score': gs4.score(X_test, y_test)\n",
    "               }})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df['difference'] = results_df['train_score'] - results_df['test_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_score</th>\n",
       "      <th>test_score</th>\n",
       "      <th>difference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>gs3_Count_Lgr</th>\n",
       "      <td>0.866048</td>\n",
       "      <td>0.869464</td>\n",
       "      <td>-0.003416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gs2_TF_NB</th>\n",
       "      <td>0.899541</td>\n",
       "      <td>0.883450</td>\n",
       "      <td>0.016091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gs4_Count_NB</th>\n",
       "      <td>0.889391</td>\n",
       "      <td>0.864802</td>\n",
       "      <td>0.024589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gs_TF_Lgr</th>\n",
       "      <td>0.897194</td>\n",
       "      <td>0.871795</td>\n",
       "      <td>0.025399</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               train_score  test_score  difference\n",
       "gs3_Count_Lgr     0.866048    0.869464   -0.003416\n",
       "gs2_TF_NB         0.899541    0.883450    0.016091\n",
       "gs4_Count_NB      0.889391    0.864802    0.024589\n",
       "gs_TF_Lgr         0.897194    0.871795    0.025399"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.sort_values(by='difference', ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All our models have easily beaten the baseline accuracy score.\n",
    "\n",
    "As expected, the TF-IDF Vectorizer models have a slightly better accuracy than Count Vectorizer ones.\n",
    "\n",
    "While the Countvectorizer + Logistic Regression model (gs3) has the smallest difference in scores between the train and test sets, the TF-IDF Vectorizer and Naive Bayes (gs2) model has a better score overall. We will take a closer look at them both."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving Features of gs3\n",
    "\n",
    "The importance of words in a Logistic Regression is determined by its coefficient, with positive coefficients indicating that the word tends a given text towards being predicted as being from r/OCD, and one with a negative coefficient, r/ADHD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = gs3.best_estimator_.named_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.03303111, -0.14355633, -0.11943444, ..., -0.07102766,\n",
       "       -0.13362273, -0.0053784 ])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.lgr.coef_.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_df = pd.DataFrame(zip(best_model.vector.get_feature_names(), best_model.lgr.coef_.ravel()),\n",
    "                      columns = ['word', 'coefficient'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At one end, the words with corresponding lowest coefficients tend the post towards being predicted as being from r/ADHD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>125</th>\n",
       "      <th>1232</th>\n",
       "      <th>3356</th>\n",
       "      <th>1040</th>\n",
       "      <th>622</th>\n",
       "      <th>1246</th>\n",
       "      <th>1840</th>\n",
       "      <th>2547</th>\n",
       "      <th>1684</th>\n",
       "      <th>812</th>\n",
       "      <th>2858</th>\n",
       "      <th>3113</th>\n",
       "      <th>1144</th>\n",
       "      <th>2514</th>\n",
       "      <th>2282</th>\n",
       "      <th>2695</th>\n",
       "      <th>394</th>\n",
       "      <th>540</th>\n",
       "      <th>3105</th>\n",
       "      <th>3008</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>word</th>\n",
       "      <td>adderall</td>\n",
       "      <td>focus</td>\n",
       "      <td>vyvanse</td>\n",
       "      <td>energy</td>\n",
       "      <td>concerta</td>\n",
       "      <td>forget</td>\n",
       "      <td>med</td>\n",
       "      <td>sadness</td>\n",
       "      <td>lack</td>\n",
       "      <td>depression</td>\n",
       "      <td>stimulant</td>\n",
       "      <td>tomorrow</td>\n",
       "      <td>fail</td>\n",
       "      <td>ritalin</td>\n",
       "      <td>process</td>\n",
       "      <td>sit</td>\n",
       "      <td>bore</td>\n",
       "      <td>class</td>\n",
       "      <td>today</td>\n",
       "      <td>task</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>coefficient</th>\n",
       "      <td>-1.23386</td>\n",
       "      <td>-1.08889</td>\n",
       "      <td>-0.992342</td>\n",
       "      <td>-0.897748</td>\n",
       "      <td>-0.87824</td>\n",
       "      <td>-0.814418</td>\n",
       "      <td>-0.788284</td>\n",
       "      <td>-0.759452</td>\n",
       "      <td>-0.757148</td>\n",
       "      <td>-0.748582</td>\n",
       "      <td>-0.718724</td>\n",
       "      <td>-0.694334</td>\n",
       "      <td>-0.692355</td>\n",
       "      <td>-0.68479</td>\n",
       "      <td>-0.679669</td>\n",
       "      <td>-0.674385</td>\n",
       "      <td>-0.673979</td>\n",
       "      <td>-0.672597</td>\n",
       "      <td>-0.668248</td>\n",
       "      <td>-0.661922</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 125      1232      3356      1040      622       1246  \\\n",
       "word         adderall    focus   vyvanse    energy  concerta    forget   \n",
       "coefficient  -1.23386 -1.08889 -0.992342 -0.897748  -0.87824 -0.814418   \n",
       "\n",
       "                 1840      2547      1684        812        2858      3113  \\\n",
       "word              med   sadness      lack  depression  stimulant  tomorrow   \n",
       "coefficient -0.788284 -0.759452 -0.757148   -0.748582  -0.718724 -0.694334   \n",
       "\n",
       "                 1144     2514      2282      2695      394       540   \\\n",
       "word             fail  ritalin   process       sit      bore     class   \n",
       "coefficient -0.692355 -0.68479 -0.679669 -0.674385 -0.673979 -0.672597   \n",
       "\n",
       "                 3105      3008  \n",
       "word            today      task  \n",
       "coefficient -0.668248 -0.661922  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " word_df.sort_values(by='coefficient', ascending=True).head(20).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And on the other end, the words with corresponding highest coefficients tend the post towards being predicted as being from r/OCD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>613</th>\n",
       "      <th>2016</th>\n",
       "      <th>2014</th>\n",
       "      <th>3070</th>\n",
       "      <th>2399</th>\n",
       "      <th>3163</th>\n",
       "      <th>2602</th>\n",
       "      <th>2387</th>\n",
       "      <th>1747</th>\n",
       "      <th>2018</th>\n",
       "      <th>3125</th>\n",
       "      <th>1173</th>\n",
       "      <th>2566</th>\n",
       "      <th>1719</th>\n",
       "      <th>2313</th>\n",
       "      <th>1693</th>\n",
       "      <th>945</th>\n",
       "      <th>2054</th>\n",
       "      <th>3059</th>\n",
       "      <th>3496</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>word</th>\n",
       "      <td>compulsion</td>\n",
       "      <td>obsession</td>\n",
       "      <td>obsess</td>\n",
       "      <td>thought</td>\n",
       "      <td>reassurance</td>\n",
       "      <td>trigger</td>\n",
       "      <td>seek</td>\n",
       "      <td>real</td>\n",
       "      <td>live</td>\n",
       "      <td>obsessive</td>\n",
       "      <td>touch</td>\n",
       "      <td>fear</td>\n",
       "      <td>scar</td>\n",
       "      <td>lexapro</td>\n",
       "      <td>prozac</td>\n",
       "      <td>lately</td>\n",
       "      <td>doubt</td>\n",
       "      <td>order</td>\n",
       "      <td>theme</td>\n",
       "      <td>zoloft</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>coefficient</th>\n",
       "      <td>1.40044</td>\n",
       "      <td>1.15391</td>\n",
       "      <td>0.993564</td>\n",
       "      <td>0.80983</td>\n",
       "      <td>0.798586</td>\n",
       "      <td>0.773463</td>\n",
       "      <td>0.760096</td>\n",
       "      <td>0.760044</td>\n",
       "      <td>0.759874</td>\n",
       "      <td>0.740649</td>\n",
       "      <td>0.652808</td>\n",
       "      <td>0.637671</td>\n",
       "      <td>0.637622</td>\n",
       "      <td>0.636135</td>\n",
       "      <td>0.624141</td>\n",
       "      <td>0.620851</td>\n",
       "      <td>0.612523</td>\n",
       "      <td>0.610499</td>\n",
       "      <td>0.606297</td>\n",
       "      <td>0.602314</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   613        2016      2014     3070         2399      3163  \\\n",
       "word         compulsion  obsession    obsess  thought  reassurance   trigger   \n",
       "coefficient     1.40044    1.15391  0.993564  0.80983     0.798586  0.773463   \n",
       "\n",
       "                 2602      2387      1747       2018      3125      1173  \\\n",
       "word             seek      real      live  obsessive     touch      fear   \n",
       "coefficient  0.760096  0.760044  0.759874   0.740649  0.652808  0.637671   \n",
       "\n",
       "                 2566      1719      2313      1693      945       2054  \\\n",
       "word             scar   lexapro    prozac    lately     doubt     order   \n",
       "coefficient  0.637622  0.636135  0.624141  0.620851  0.612523  0.610499   \n",
       "\n",
       "                 3059      3496  \n",
       "word            theme    zoloft  \n",
       "coefficient  0.606297  0.602314  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_df.sort_values(by='coefficient', ascending=False).head(20).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save words against cofficients for comparison in the next section\n",
    "word_df.to_csv('../datasets/words_lem_count_lgr.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving Features from gs2\n",
    "\n",
    "Unlike Logistic Regression, Naive Bayes trains a model by assigning each word feature a probability assuming that it is from r/OCD or r/ADHD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = gs2.best_estimator_.named_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-8.69043092, -8.79874769, -6.87536537, ..., -7.90663342,\n",
       "        -7.49730996, -8.1246333 ],\n",
       "       [-9.36682958, -9.36682958, -7.23623583, ..., -9.36682958,\n",
       "        -8.40317939, -8.29434558]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.nb.feature_log_prob_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The log probabilities above indicate the probability that we saw each corresponding word given that it was a post from r/OCD and r/ADHD respectively. We will save the top 100 of these words and analyze them in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the log probabilities sorted from the largest to to smallest for each class.\n",
    "neg_class_prob_sorted = best_model.nb.feature_log_prob_[0, :].argsort()[::-1]\n",
    "pos_class_prob_sorted = best_model.nb.feature_log_prob_[1, :].argsort()[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the top 100 feature words for the r/ADHD class with its corresponding log probabilities.\n",
    "top_100_adhd_words = pd.DataFrame(\n",
    "    zip(\n",
    "    np.take(best_model.vector.get_feature_names(), neg_class_prob_sorted[:100]),\n",
    "     np.sort(best_model.nb.feature_log_prob_[1, :])[::-1][:100]\n",
    "    ),\n",
    "    columns=['word','probability'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>word</th>\n",
       "      <td>work</td>\n",
       "      <td>day</td>\n",
       "      <td>med</td>\n",
       "      <td>medication</td>\n",
       "      <td>help</td>\n",
       "      <td>really</td>\n",
       "      <td>try</td>\n",
       "      <td>start</td>\n",
       "      <td>want</td>\n",
       "      <td>need</td>\n",
       "      <td>...</td>\n",
       "      <td>prescribe</td>\n",
       "      <td>self</td>\n",
       "      <td>stop</td>\n",
       "      <td>leave</td>\n",
       "      <td>guy</td>\n",
       "      <td>dose</td>\n",
       "      <td>minute</td>\n",
       "      <td>tip</td>\n",
       "      <td>motivation</td>\n",
       "      <td>fuck</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>probability</th>\n",
       "      <td>-4.82616</td>\n",
       "      <td>-5.40174</td>\n",
       "      <td>-5.43788</td>\n",
       "      <td>-5.48572</td>\n",
       "      <td>-5.48892</td>\n",
       "      <td>-5.56176</td>\n",
       "      <td>-5.61237</td>\n",
       "      <td>-5.62541</td>\n",
       "      <td>-5.6843</td>\n",
       "      <td>-5.72841</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.6204</td>\n",
       "      <td>-6.62048</td>\n",
       "      <td>-6.62219</td>\n",
       "      <td>-6.6309</td>\n",
       "      <td>-6.63636</td>\n",
       "      <td>-6.63884</td>\n",
       "      <td>-6.63906</td>\n",
       "      <td>-6.64305</td>\n",
       "      <td>-6.64432</td>\n",
       "      <td>-6.64795</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0        1        2           3        4        5        6   \\\n",
       "word            work      day      med  medication     help   really      try   \n",
       "probability -4.82616 -5.40174 -5.43788    -5.48572 -5.48892 -5.56176 -5.61237   \n",
       "\n",
       "                  7       8        9   ...         90       91       92  \\\n",
       "word           start    want     need  ...  prescribe     self     stop   \n",
       "probability -5.62541 -5.6843 -5.72841  ...    -6.6204 -6.62048 -6.62219   \n",
       "\n",
       "                 93       94       95       96       97          98       99  \n",
       "word          leave      guy     dose   minute      tip  motivation     fuck  \n",
       "probability -6.6309 -6.63636 -6.63884 -6.63906 -6.64305    -6.64432 -6.64795  \n",
       "\n",
       "[2 rows x 100 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_100_adhd_words.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the same for r/OCD.\n",
    "top_100_ocd_words = pd.DataFrame(\n",
    "    zip(\n",
    "    np.take(best_model.vector.get_feature_names(), pos_class_prob_sorted[:100]),\n",
    "     np.sort(best_model.nb.feature_log_prob_[1, :])[::-1][:100]\n",
    "    ),\n",
    "    columns=['word','probability'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>word</th>\n",
       "      <td>thought</td>\n",
       "      <td>intrusive</td>\n",
       "      <td>want</td>\n",
       "      <td>say</td>\n",
       "      <td>bad</td>\n",
       "      <td>really</td>\n",
       "      <td>happen</td>\n",
       "      <td>people</td>\n",
       "      <td>compulsion</td>\n",
       "      <td>try</td>\n",
       "      <td>...</td>\n",
       "      <td>live</td>\n",
       "      <td>die</td>\n",
       "      <td>lose</td>\n",
       "      <td>hate</td>\n",
       "      <td>enjoy</td>\n",
       "      <td>little</td>\n",
       "      <td>kind</td>\n",
       "      <td>convince</td>\n",
       "      <td>ago</td>\n",
       "      <td>watch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>probability</th>\n",
       "      <td>-4.82616</td>\n",
       "      <td>-5.40174</td>\n",
       "      <td>-5.43788</td>\n",
       "      <td>-5.48572</td>\n",
       "      <td>-5.48892</td>\n",
       "      <td>-5.56176</td>\n",
       "      <td>-5.61237</td>\n",
       "      <td>-5.62541</td>\n",
       "      <td>-5.6843</td>\n",
       "      <td>-5.72841</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.6204</td>\n",
       "      <td>-6.62048</td>\n",
       "      <td>-6.62219</td>\n",
       "      <td>-6.6309</td>\n",
       "      <td>-6.63636</td>\n",
       "      <td>-6.63884</td>\n",
       "      <td>-6.63906</td>\n",
       "      <td>-6.64305</td>\n",
       "      <td>-6.64432</td>\n",
       "      <td>-6.64795</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0          1        2        3        4        5        6   \\\n",
       "word         thought  intrusive     want      say      bad   really   happen   \n",
       "probability -4.82616   -5.40174 -5.43788 -5.48572 -5.48892 -5.56176 -5.61237   \n",
       "\n",
       "                  7           8        9   ...      90       91       92  \\\n",
       "word          people  compulsion      try  ...    live      die     lose   \n",
       "probability -5.62541     -5.6843 -5.72841  ... -6.6204 -6.62048 -6.62219   \n",
       "\n",
       "                 93       94       95       96        97       98       99  \n",
       "word           hate    enjoy   little     kind  convince      ago    watch  \n",
       "probability -6.6309 -6.63636 -6.63884 -6.63906  -6.64305 -6.64432 -6.64795  \n",
       "\n",
       "[2 rows x 100 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_100_ocd_words.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save words against log probabilities for comparison in the next section\n",
    "top_100_adhd_words.to_csv('../datasets/words_adhd_lem_tf_nb.csv', index=False)\n",
    "top_100_ocd_words.to_csv('../datasets/words_ocd_lem_tf_nb.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": "5",
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
